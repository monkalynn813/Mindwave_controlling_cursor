{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg1 = pd.read_csv(\"eeg1.csv\", delimiter=\"\\t\")\n",
    "new_columns = eeg1.columns.values \n",
    "new_columns[0] = 'time'     \n",
    "new_columns[33] = 'sample' \n",
    "eeg1.columns = new_columns\n",
    "\n",
    "events1 = pd.read_csv(\"events1.csv\") #, delimiter=\"\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# takes in eeg dataframe and event dataframe, cleans them, 1hot encodes the events\n",
    "def clean_eeg(eeg, events, event_interval_length, eeg_slice_length):\n",
    "    #event_list = []\n",
    "    array_list = [] \n",
    "    index_list = []\n",
    "    eeg = standardize_eeg(eeg) # function for standardizing the eeg readings\n",
    "    #events_new = build_zero_events(events)\n",
    "    # iterate over the rows of the events and slice out the corresponding eeg data\n",
    "    for index, row in itertools.islice(events.iterrows(), event_interval_length): # loop through events data\n",
    "        #build_event_list(row, event_list) #\n",
    "        tmin, tmax = build_event_intervals(row, events)\n",
    "        eeg_slice = cut_event_intervals(eeg, tmin, tmax)\n",
    "        array_list, index_list = build_array(eeg_slice, eeg_slice_length, \n",
    "                                             index, index_list, array_list)\n",
    "    y_int = events.iloc[index_list] # take the event types for the correct index\n",
    "    y_int = y_int['type'].values    # take just the event types as an array\n",
    "    #y_int = y_int.as_matrix()            # save the event types as a matrix\n",
    "    #y, lb = one_hot_events(y_int)        # one-hot the event types and save the binarizer\n",
    "    X = np.stack(array_list, axis = 0)   # stack the arrays so the whole thing is 3D\n",
    "    return X, y_int                     # return the data, outputs, and the binarizer\n",
    "    \n",
    "        \n",
    "def build_event_list(row, event_list):\n",
    "    # helper function to pull event types out of event data in the right order\n",
    "    event_type = getattr(row, \"type\")\n",
    "    event_list.append(event_type)\n",
    "        \n",
    "def build_event_intervals(row, events):\n",
    "    # helper function to get the time intervals associated with each event\n",
    "    tmin = getattr(row, \"latency\")\n",
    "    tmin_in = getattr(row, \"number\")\n",
    "    tmax_in = tmin_in + 1\n",
    "    tmax = events1.loc[tmax_in, \"latency\"]\n",
    "    return tmin, tmax\n",
    "\n",
    "def cut_event_intervals(eeg, tmin, tmax):\n",
    "    # helper function to slice up the eeg data so each slice is associated with one event\n",
    "    eeg_slice = eeg.loc[(eeg[\"time\"] > tmin) & (eeg[\"time\"] < tmax)]\n",
    "    eeg_slice.drop([\"time\", \"sample\"], axis = 1, inplace = True)\n",
    "    return eeg_slice\n",
    "    \n",
    "def build_array(eeg_slice, eeg_slice_length, index, index_list, array_list):\n",
    "    # helper function to build an array out of the eeg slices and pad them out to a standard length\n",
    "    if len(eeg_slice) < eeg_slice_length:\n",
    "        index_list.append(index)\n",
    "        eeg_matrix = eeg_slice.as_matrix()\n",
    "        padded_matrix = np.pad(eeg_matrix, ((0, eeg_slice_length - len(eeg_matrix)), (0,0)),\n",
    "                                   'constant', constant_values=0)\n",
    "        array_list.append(padded_matrix)\n",
    "    return array_list, index_list\n",
    "\n",
    "def one_hot_events(events):\n",
    "    # helper function for one-hot encoding the events\n",
    "    events_list = list(events)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(events_list)\n",
    "    events_1hot = lb.transform(events_list)\n",
    "    return events_1hot, lb\n",
    "\n",
    "def invert_one_hot(events, lb):\n",
    "    # function for decoding one-hot, binarizer made in one_hot_events\n",
    "    inv_events = lb.inverse_transform(events)\n",
    "    return inv_events\n",
    "def standardize_eeg(eeg_data):\n",
    "    # breaks apart an eeg dataframe, scales the eeg readings, and reassmbles it into a dataframe\n",
    "    column_list = eeg_data.columns[1:33]\n",
    "    time = eeg_data['time']\n",
    "    sample = eeg_data['sample']\n",
    "    eeg_array = eeg_data[column_list]\n",
    "    eeg_stnd = scale_data(eeg_array)\n",
    "    eeg_stnd_df = pd.DataFrame(eeg_stnd, index=eeg_data.index, columns=column_list)\n",
    "    eeg_stnd = pd.concat([time, eeg_stnd_df, sample], axis =1)\n",
    "    return eeg_stnd\n",
    "\n",
    "def scale_data(unscaled_data):\n",
    "    # helper function for standardize_eeg, fits a scaler and transforms the data \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(unscaled_data)\n",
    "    scaled_data = scaler.transform(unscaled_data)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# full dataset parameters\n",
    "\n",
    "# define model parameters\n",
    "samples = 3625  # how many trials of eeg data\n",
    "n_features = 32  # how many channels of eeg in each sample\n",
    "time_steps = 1300 # how many ms was each sample run for\n",
    "event_types = 2 #len(set(y))  # how many different event types (light, sound, etc) are there # 6 large, 4 smol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = clean_eeg(eeg1, events1, samples, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# couple thousand examples\n",
    "remove_list = [0,2,4,5,6]              # designate unwanted event types\n",
    "drop_list = np.isin(y, remove_list)    # create a list of indices associated with unwanted events                  \n",
    "drop_array = np.array(drop_list) \n",
    "\n",
    "# make X, y's with the unwanted events removed\n",
    "y_short_int = y[np.isin(y,remove_list, invert=True)]\n",
    "X_short = X[np.isin(y, remove_list, invert=True)]\n",
    "y_short, lb = one_hot_events(y_short_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "# use strat. shuffle split to get indices for test and training data \n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=seed)\n",
    "sss.get_n_splits(X_short, y_short)\n",
    "for train_index, test_index in sss.split(X_short, y_short):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_short[train_index], X_short[test_index]\n",
    "    y_train, y_test = y_short[train_index], y_short[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# code for building an LSTM with 100 neurons and dropout. Runs for 50 epochs\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, return_sequences=False, input_shape=(time_steps, n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(LSTM(100)) dramatically worse results\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=1)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
